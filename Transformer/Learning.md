# What is tranformer:
Ans: It one of the type of neural network it generate and Process the data such as text and speech.

# How it is different from others Architecture.
Ans: Some features that differentiate transformer to other Architecture.

1. Self-Attention Mechanism: It is a technique used in tranformer by which model understand the realtionship between words ina sentence.Self-attenion looks at all words at once in a sequence at once.
    **For Example:** Abid sit on the chair because he was tired. 

    The model learn what he refers ,he refers to abid. 
    

2. Parallelization:
Process things parallel

3. Positional Encoding:provide information about the position or order of elements in a sequence. 

4. Multi Head Attention: focus on multiple words at the same time

5. Feedforward layers:Information flows in one directionâ€”from the input layer, through hidden layers, to the output layer.

6. Layer Normalization:
Helps the model train faster and work more efficiently.

It enables the parallelization and long range depedencies by which context remenbering is easy.

# How Transformer Works:
Ans: Transformer Consist an Encoder and decoder structure.

Encoder: It Reads and Process the input.

Decoder: uses encoder outputs to generate predictions step byt step.

